{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564348bd",
   "metadata": {},
   "source": [
    "# 3. Method 2: De Novo Sequencing with Transformers (Casanovo)\n",
    "\n",
    "## 3.1 The Problem: What if there's no database?\n",
    "\n",
    "In the previous notebook, we used **spectral hashing** to cluster spectra and match them against a database. But what happens when:\n",
    "- You're studying a new organism with no reference proteome?\n",
    "- You want to discover novel peptides or variants not in any database?\n",
    "- You need to identify antibodies or other highly variable sequences?\n",
    "\n",
    "**De novo sequencing** solves this by predicting the peptide sequence directly from the spectrum — no database required.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 The Approach: Sequence-to-Sequence Translation\n",
    "\n",
    "Casanovo treats peptide identification as a **translation problem**:\n",
    "\n",
    "| Language Translation | De Novo Sequencing |\n",
    "|---------------------|-------------------|\n",
    "| Input: English sentence | Input: MS/MS spectrum (peaks) |\n",
    "| Output: Spanish sentence | Output: Peptide sequence (amino acids) |\n",
    "| \"The cat sat on the mat\" → \"El gato se sentó en la alfombra\" | [peaks at 147.1, 260.2, ...] → \"PEPTIDE\" |\n",
    "\n",
    "A translator must understand grammar, word relationships, and context, and so Casanovo must understand:\n",
    "- **Peak relationships**: Which peaks are b-ions vs y-ions? Which are noise?\n",
    "- **Mass differences**: A gap of 129 Da suggests glutamic acid (E)\n",
    "- **Sequence context**: Certain amino acids are more likely to follow others\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 The (Extended) Lego Analogy: Reading Assembly Instructions Backwards\n",
    "\n",
    "Imagine you find a completed Lego structure, but you've lost the instructions. You need to figure out the **original build sequence**.\n",
    "\n",
    "**The naive approach:**\n",
    "- Look at the final structure\n",
    "- Try every possible disassembly order until you find one that makes sense\n",
    "\n",
    "**The Casanovo approach:**\n",
    "- Look at the \"joints\" between pieces (like mass differences between peaks)\n",
    "- Use pattern recognition: \"2x4 bricks usually connect to flat plates here\"\n",
    "- Literally build up the sequence, one brick at a time, with pattern recognition as \"intuition\"\n",
    "\n",
    "> Just like an experienced Lego builder recognizes common substructures, Casanovo learns patterns from millions of spectra to predict the most likely next amino acid.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Why Positional Encoding Matters\n",
    "\n",
    "The question we will solve is, **How does Casanovo learn where each peak is?**\n",
    "- What understanding does it have about this peak, and peaks relative to this one? \n",
    "\n",
    "When you read \"THE CAT SAT\", the position of each word matters:\n",
    "- \"CAT THE SAT\" is nonsense\n",
    "- Position encodes meaning\n",
    "\n",
    "Similarly, for spectra:\n",
    "- A peak at m/z = 147 means something different than a peak at m/z = 647\n",
    "- The *absolute position* (m/z value) and *relative positions* (gaps between peaks) both carry information\n",
    "\n",
    "**Positional encoding** is how we teach the model to understand position. It converts each m/z value into a rich, multi-dimensional representation that captures both:\n",
    "1. **Fine-scale differences**: Distinguishing m/z = 600.0 from m/z = 600.1 (isotope patterns)\n",
    "2. **Coarse-scale structure**: Understanding that m/z = 200 and m/z = 800 are in different regions of the spectrum\n",
    "\n",
    "This is easily represented with visuals, and we'll understand this concept with toy examples, as well as code implementations later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152cfb9",
   "metadata": {},
   "source": [
    "## 3.5 The Full Pipeline: From Spectrum to Peptide\n",
    "\n",
    "Before diving into the math, let's get a full view of what Casanovo does:\n",
    "\n",
    "```\n",
    "┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐     ┌────────────┐\n",
    "│   MS/MS Peaks   │ ──► │ Positional       │ ──► │   Transformer   │ ──► │  Peptide   │\n",
    "│  (m/z, int)     │     │ Encoding         │     │   Decoder       │     │  Sequence  │\n",
    "└─────────────────┘     └──────────────────┘     └─────────────────┘     └────────────┘\n",
    "     [147.1, ...]           [512-dim vectors]        attention +            \"PEPTIDEK\"\n",
    "                                                     beam search\n",
    "```\n",
    "\n",
    "**Step 1: Positional Encoding** (this notebook's focus)\n",
    "- Each peak's m/z value is transformed into a 512-dimensional vector\n",
    "- Uses sine and cosine waves at multiple frequencies (wavelengths)\n",
    "- Captures both fine and coarse positional information\n",
    " \n",
    "**Step 2: Transformer Encoder-Decoder**\n",
    "- Encoder: Processes all peak encodings together, learning relationships\n",
    "- Decoder: Generates amino acids one at a time, conditioned on previous predictions\n",
    "\n",
    "**Step 3: Beam Search**\n",
    "- Instead of greedily picking the most likely amino acid, keeps top-k candidates\n",
    "- Prunes candidates that exceed precursor mass tolerance\n",
    "- Outputs the highest-probability complete sequence\n",
    "\n",
    "---\n",
    "\n",
    "Let's dive into step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5870373",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part I: Understanding Positional Encoding\n",
    "\n",
    "## 3.6 The Math Behind the Waves\n",
    "\n",
    "Casanovo's positional encoding transforms each m/z value into a 512-dimensional vector using sine and cosine waves. Let's build up to the full equation step by step. This is really not as complicated as it sounds.\n",
    "\n",
    "![Each m/z value, m_j, is projected into 512 dimensions](CasanovoEncoding.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Do you remember this equation from high school? It's the general form of a sine wave:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116fcf6",
   "metadata": {},
   "source": [
    "$$\n",
    "y = A\\sin\\big(B(x - h)\\big) + k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Amplitude} &= |A|, \\\\\n",
    "\\text{Midline} &= y=k, \\\\\n",
    "\\text{Period} &= \\frac{2\\pi}{|B|}\\ \\text{ (or } \\frac{360^\\circ}{|B|}\\text{)}, \\\\\n",
    "\\text{Phase shift} &= \n",
    "\\begin{cases}\n",
    "\\text{right } h & \\text{if } (h < 0),\\\\\n",
    "\\text{left } h & \\text{if } (h > 0).\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.6.1 A Numerical Example\n",
    "\n",
    "Consider this sine wave: $\\sin\\left(\\frac{x}{(0.001/2\\pi)(10,000/0.001)^{1/2}}\\right)$\n",
    "\n",
    "We can calculate the period:\n",
    "$$ \n",
    "\\text{Period} = \\frac{2\\pi}{B} \\text{ where } B = \\frac{1}{\\left(\\frac{0.001}{2\\pi}\\right)\\left(\\frac{10000}{0.001}\\right)^{1/2}} = 2\\pi \\times \\left(\\frac{0.001}{2\\pi}\\right) \\times 3162.277 = 3.162277\n",
    "$$\n",
    "\n",
    "So this sine wave completes one full cycle every ~3.16 m/z units.\n",
    "\n",
    "![A single period plotted for this sine wave](desmos_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 3.6.2 The Wavelength Concept\n",
    "\n",
    "Now imagine a list of 256 wavelengths $\\lambda_i$ spanning from $0.001$ to $10000$, increasing exponentially:\n",
    "$$\\lambda = [0.001, \\dots, 10000]$$\n",
    "\n",
    "Each wavelength defines a different \"ruler\" for measuring position — some with very fine ticks (short wavelengths), others with very coarse ticks (long wavelengths).\n",
    "\n",
    "> **Lego analogy:** Think of wavelengths as different measuring tools:\n",
    "> - Short wavelength = calipers (precise to 0.1mm) — distinguishes similar-sized pieces\n",
    "> - Long wavelength = arm span — tells you \"big piece\" vs \"small piece\"\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7 Breaking Down the Formula\n",
    "\n",
    "Let's focus on just the sine component:\n",
    "\n",
    "$$\n",
    "f_i(m_j) = \\sin\\left(\\frac{m_j}{(\\lambda_{\\min}/2\\pi)\\times(\\lambda_{\\max}/\\lambda_{\\min})^{i/(\\dim-1)}}\\right) \\quad \\text{for } i < 256\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m_j$ is the $j$-th peak's m/z value\n",
    "- $\\dim = 512$ (total dimensions: 256 sine + 256 cosine)\n",
    "- $\\lambda_{\\min} = 0.001$ and $\\lambda_{\\max} = 10000$\n",
    "\n",
    "The denominator simplifies to our wavelength $\\lambda_i$:\n",
    "\n",
    "$$\n",
    "f_i(m_j) = \\sin\\left(\\frac{m_j}{\\lambda_i}\\right) \\quad \\text{where } \\lambda_i = \\left(\\frac{\\lambda_{\\min}}{2\\pi}\\right)\\left(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\\right)^{\\frac{i}{d_{\\sin}-1}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.7.1 Why Include $2\\pi$ in the Formula?\n",
    "\n",
    "The $2\\pi$ in the denominator is elegant! When we calculate the period:\n",
    "\n",
    "$$\n",
    "\\text{Period} = 2\\pi \\times \\lambda_i = 2\\pi \\times \\left(\\frac{\\lambda_{\\min}}{2\\pi}\\right)\\left(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\\right)^{\\frac{i}{d_{\\sin}-1}}\n",
    "$$\n",
    "\n",
    "The $2\\pi$ terms cancel, giving us:\n",
    "\n",
    "$$\n",
    "\\text{Period} = \\lambda_{\\min} \\times \\left(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\\right)^{\\frac{i}{d_{\\sin}-1}}\n",
    "$$\n",
    "\n",
    "As $i$ increases from 0 to 255, the period grows exponentially from $\\lambda_{\\min}$ to $\\lambda_{\\max}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.7.2 Low-Dimensional Example\n",
    "\n",
    "Let's compute 6 wavelengths (instead of 256) to see the pattern:\n",
    "\n",
    "$\\lambda_i = \\frac{\\lambda_{\\min}}{2\\pi} \\times \\left(\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\\right)^{\\frac{i}{5}}$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\lambda_0 &= \\frac{0.001}{2\\pi} \\times 1 = \\frac{0.001}{2\\pi} \\\\\n",
    "\\lambda_1 &= \\frac{0.001}{2\\pi} \\times 15.85 = \\frac{0.0158}{2\\pi} \\\\\n",
    "\\lambda_2 &= \\frac{0.001}{2\\pi} \\times 251.19 = \\frac{0.251}{2\\pi} \\\\\n",
    "\\lambda_3 &= \\frac{0.001}{2\\pi} \\times 3981.07 = \\frac{3.981}{2\\pi} \\\\\n",
    "\\lambda_4 &= \\frac{0.001}{2\\pi} \\times 63095.73 = \\frac{63.095}{2\\pi} \\\\\n",
    "\\lambda_5 &= \\frac{0.001}{2\\pi} \\times 1000000 = \\frac{1000}{2\\pi}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice the exponential growth! Each wavelength is roughly 10-16× larger than the previous one.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.7.3 Visualizing the Waves\n",
    "\n",
    "Here are 5 sine waves at different wavelengths (notice we ommitted $\\lambda_0$)\n",
    "\n",
    "<img src=\"image2.png\" alt=\"Multiple wavelengths\" width=\"1200\"/>\n",
    "\n",
    "Zooming in:\n",
    "\n",
    "<img src=\"image.png\" alt=\"Zoomed wavelengths\" width=\"500\"/>\n",
    "\n",
    "At m/z = 600, each wave gives a different value. ($\\lambda_0 \\text{ and } \\lambda_1$ (red) ommited for visibility)\n",
    "\n",
    "![Waves at m/z = 600](image3.png)\n",
    "\n",
    "> **Understanding check:** Why did we exclude the red wave, $\\lambda_0$ from the plot?\n",
    ">\n",
    "> <details>\n",
    "> <summary>Click to see answer</summary>\n",
    "> \n",
    "> The wavelength at $i=0$ is $\\lambda_0 = \\frac{0.001}{2\\pi}$, which is extremely small. This creates a sine wave that oscillates so rapidly at typical m/z values (like 600) that it becomes practically invisible — just a blur.\n",
    "> </details>\n",
    "\n",
    "**Desmos link for all graphs:** https://www.desmos.com/calculator/he8fjmng6v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27db40",
   "metadata": {},
   "source": [
    "## 3.8 How Waves Capture Position: Fine vs. Coarse Resolution\n",
    "\n",
    "Let's trace what happens as we move from m/z = 600 to 601:\n",
    "\n",
    "![600 to 601](relativeDifferences.png)\n",
    "\n",
    "Take note of how the blue \"line\" BARELY moves cyclically? And the Red/Green waves cycle all the time? This allows us to take note of how close (only different by Red/Green) or how far(different by Black/Blue, else just noise) one peak is from another.\n",
    "\n",
    "| Wave | Period | Cycles from 600→601 | Sensitivity |\n",
    "|------|--------|---------------------|-------------|\n",
    "| Red (finest) | ~0.016! | ~63 cycles | Detects tiny shifts |\n",
    "| Green | ~0.25 | ~4 cycles | Detects small shifts |\n",
    "| Purple | ~3.98 | ~0.25 cycles | Detects moderate shifts |\n",
    "| Black | ~63.1 | ~0.016 cycles | Detects large shifts |\n",
    "| Blue (coarsest) | ~1000 | ~0.001 cycles | Distinguishes spectrum regions |\n",
    "\n",
    "**The multi-scale intuition:**\n",
    "- **Fine waves** (short periods) = microscope → distinguish isotopes, detect noise\n",
    "- **Coarse waves** (long periods) = map → identify spectrum region, peptide class\n",
    "\n",
    "> **Lego analogy:** \n",
    "> - Fine wavelengths distinguish a 2×4 brick from a 2×3 brick (subtle differences)\n",
    "> - Coarse wavelengths distinguish a wheel from a minifigure (massive, obvious differences)\n",
    "\n",
    "Here's the encoding for m/z = 600 vs 601 at each wavelength:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Peak } m/z = 600: \\quad &[\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4, \\lambda_5] \\\\\n",
    "&= [0.364, -0.782, -0.973, -0.059, -0.588] \\\\[0.5em]\n",
    "\\text{Peak } m/z = 601: \\quad &[\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4, \\lambda_5] \\\\\n",
    "&= [-0.227, -0.703, -0.222, -0.158, -0.593]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice how the fine wavelengths ($\\lambda_1$, $\\lambda_2$) change dramatically, while coarse wavelengths ($\\lambda_5$) barely move!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363cba6",
   "metadata": {},
   "source": [
    "## 3.9 Why Both Sine AND Cosine?\n",
    "\n",
    "You may have noticed the encoding uses both sine and cosine. This isn't just for more dimensions, it enables something crucial: **linear transformations between positions**.\n",
    "\n",
    "Throughout this explanation, you may have seen various references to \"we'll just focus on the sine part of the equation for now\" (see full positional encoding equation image for reference). The reason we use both sine and cosine is elegant: it allows the model to compute relative positions using simple linear algebra.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.9.1 Mathematical Insight\n",
    "\n",
    "For the model to understand relative positions, it needs to compute transformations like:\n",
    "$$A^{(k)} \\cdot \\text{PE}(m_j) = \\text{PE}(m_j + k)$$\n",
    "\n",
    "Where $A^{(k)}$ is a matrix that depends only on the offset $k$, not the position $m_j$.\n",
    "\n",
    "**Why this matters:** Whether you're at m/z = 100 or m/z = 1000, the transformation \"shift by 50 Da\" uses the same matrix. This gives the model a universal way to understand relative distances.\n",
    "\n",
    "**Basic intuition:** Encoding with sine AND cosine, not one or the other, allows us to make a transformation from one peak to peak + $k$ for some constant $k$. The model needs this transformation (represented as a matrix) to understand relative differences between points. In a nonrigorous sense, if we have points $a$, $b$, and $c$, all different by $k$, such that $a+k = b$, $b+k = c$, and $a + 2k = c$, then our model understands that these points are the same distance apart because they differ by $k$. Essentially, we rewrite our points as offsets to that point. \n",
    "\n",
    "$A^{(k)}$ is also invertible, meaning that taking $(A^{(k)})^{-1}$ allows you to reverse the transformation and shift backwards by $k$ units.\n",
    "\n",
    "> **Lego analogy:** If piece A is 3 studs to the right of piece B, and piece C is 3 studs to the right of piece D, the model understands both relationships the same way — without needing to know where on the baseplate they are.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 3.9.2 Why Sine Alone Fails\n",
    "\n",
    "If we only had the sine component $\\begin{bmatrix} \\sin(m_j/\\lambda_i) \\\\ 0 \\end{bmatrix}$, we'd be trying to represent a 2D rotation using only a 1D line (the x-axis). You can't rotate a point if you've collapsed it onto a single axis—the transformation becomes \"degenerate\".\n",
    "\n",
    "So now why do we actually need both sine AND cosine? We don't want to discard one of our axes. Take $\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$ on the cartesian plane. This is embedded with both $x$ and $y$ axes. If we just had $\\begin{bmatrix} x \\\\ 0 \\end{bmatrix}$ and wanted to get to $\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$, how would we get there? Well, we don't know, because we don't know our $x$ value. Now if we have $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and wanted to get to $\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$, that requires $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$. Discarding the cosine values is analogous to discarding the x axis.\n",
    "\n",
    "\n",
    "**The Rotation Requires Two Coordinates:**\n",
    "Think of each wavelength $\\lambda_i$ as defining a 2D plane. At wavelength $\\lambda_i$, we encode position $m_j$ as a point in 2D space:\n",
    "\n",
    "- For each peak $m_j$:\n",
    "    - For each wavelength $\\lambda_i$:\n",
    "        - Encode the peak at wavelength $i$ in 2 dimensions: sine and cosine\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sin(m_j/\\lambda_i) \\\\\n",
    "\\cos(m_j/\\lambda_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This point lies on the unit circle. Moving from position $m_j$ to position $m_j + k$ corresponds to rotating this point by angle $k/\\lambda_i$ around the origin.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3.9.3 [Optional] The Rotation Matrix\n",
    "\n",
    "We need to show that $A^{(k)}$ is a **rotation matrix**. For each wavelength $\\lambda_i$, the corresponding 2×2 block in $A^{(k)}$ looks like:\n",
    "\n",
    "$$\n",
    "A^{(k)} = \\begin{bmatrix}\n",
    "\\cos(k/\\lambda_i) & -\\sin(k/\\lambda_i) \\\\\n",
    "\\sin(k/\\lambda_i) & \\cos(k/\\lambda_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is the standard 2D rotation matrix that rotates a point by angle $\\theta = k/\\lambda_i$. Here's why this only works when we have **both** sine and cosine:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 3.9.4 [Optional] The Math: Rotation Matrix Multiplication\n",
    "\n",
    "When we multiply the rotation matrix $A^{(k)}$ by our encoding vector, we get: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\cos(k/\\lambda_i) & -\\sin(k/\\lambda_i) \\\\\n",
    "\\sin(k/\\lambda_i) & \\cos(k/\\lambda_i)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sin(m_j/\\lambda_i) \\\\\n",
    "\\cos(m_j/\\lambda_i)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sin(m_j/\\lambda_i)\\cos(k/\\lambda_i) - \\cos(m_j/\\lambda_i)\\sin(k/\\lambda_i) \\\\\n",
    "\\sin(m_j/\\lambda_i)\\sin(k/\\lambda_i) + \\cos(m_j/\\lambda_i)\\cos(k/\\lambda_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using the angle addition formulas:\n",
    "$$\\sin(a+b) = \\sin(a)\\cos(b) + \\cos(a)\\sin(b)$$\n",
    "$$\\cos(a+b) = \\cos(a)\\cos(b) - \\sin(a)\\sin(b)$$\n",
    "\n",
    "This turns into:\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sin((m_j + k)/\\lambda_i) \\\\\n",
    "\\cos((m_j + k)/\\lambda_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Notice that to compute $\\sin((m_j + k)/\\lambda_i)$, the formula requires **both** $\\sin(m_j/\\lambda_i)$ and $\\cos(m_j/\\lambda_i)$. If we only encoded sine values, the $\\cos(m_j/\\lambda_i)$ term wouldn't exist, making the transformation a) nonlinear or b) dependent on the current position $m_j$, which contradicts the proposition that we create a matrix $A^{(k)}$ independent of $m_j$.\n",
    "\n",
    "**Full proof:** [Linear Relationships in Positional Encoding](https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba18756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9185d0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part II: Implementing Positional Encoding\n",
    "\n",
    "## 3.10 Code Implementation\n",
    "\n",
    "The intuition + math was the hard part. Let's implement the positional encoding (in ~10 lines of code) and visualize how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43675ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b97d27",
   "metadata": {},
   "source": [
    "Note that both sine and cosine encodings use the same list of wavelengths. A single set of 256 wavelengths is applied to compute both the sine and cosine components, giving us 512 total dimensions (256 sine + 256 cosine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wavelength_list(dimensions: int, lambda_min: float = 0.001, lambda_max: float = 10000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate wavelengths following the Casanovo formula:\n",
    "    ꟛ_i = (ꟛ_min / 2π) * (ꟛ_max / ꟛ_min)^(i / (d_sin - 1))\n",
    "    \n",
    "    Args:\n",
    "        d_model: Total dimensions (split evenly between sin/cos)\n",
    "        lambda_min: Minimum wavelength (default: 0.001, as in Casanovo)\n",
    "        lambda_max: Maximum wavelength (default: 10000, as in Casanovo)\n",
    "    Returns: \n",
    "        Array of d_model // 2 wavelengths\n",
    "    \"\"\"\n",
    "    d_sin = dimensions // 2\n",
    "    wavelengths = np.zeros(d_sin)\n",
    "    for i in range(d_sin):\n",
    "        wavelengths[i] = (lambda_min / (2 * np.pi)) * (lambda_max / lambda_min) ** (i / (d_sin - 1))\n",
    "    return wavelengths\n",
    "\n",
    "# Let's see what wavelengths look like for 512 dimensions\n",
    "wavelengths = make_wavelength_list(512)\n",
    "print(f\"Number of wavelengths: {len(wavelengths)}\")\n",
    "print(f\"Smallest wavelength: {wavelengths[0]:.6f}\")\n",
    "print(f\"Largest wavelength: {wavelengths[-1]:.6f}\")\n",
    "print(f\"\\nFirst 10 wavelengths: {wavelengths[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positional encoding function used by Casanovo\n",
    "# This function takes in 1 m/z value and returns a 512-dimensional positional encoding vector\n",
    "# (this feels sort of like a reverse Fourier transform)\n",
    "def positional_encoding(m_z, dimensions=512, lambda_min=0.001, lambda_max=10000):\n",
    "    \"\"\"\n",
    "    Encode a single m/z value into a d_model-dimensional vector.\n",
    "    \n",
    "    First d_model/2 dimensions: sine encoding\n",
    "    Last d_model/2 dimensions: cosine encoding\n",
    "        \n",
    "    Args:\n",
    "        m_z: The m/z value to encode\n",
    "        d_model: Total dimensionality of the encoding\n",
    "        lambda_min, lambda_max: Wavelength range\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (d_model,)\n",
    "    \"\"\"\n",
    "    encoding = np.zeros(dimensions)\n",
    "    d_sin = dimensions // 2\n",
    "    wavelengths = make_wavelength_list(dimensions)\n",
    "    for d in range(d_sin):\n",
    "        wavelength = wavelengths[d]\n",
    "        encoding[d] = np.sin(m_z / wavelength)           # First half: sine\n",
    "        encoding[d + d_sin] = np.cos(m_z / wavelength)   # Second half: cosine\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bbb28",
   "metadata": {},
   "source": [
    "Let's go through a small, but very revealing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a012d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode three different m/z values\n",
    "encode_1000 = positional_encoding(1000)\n",
    "encode_1000_1 = positional_encoding(1000.1)  # Very close to 1000\n",
    "encode_126 = positional_encoding(126.127)    # Very different from 1000\n",
    "\n",
    "# Check similarity using dot product\n",
    "print(\"Similarity (dot product) comparisons:\")\n",
    "print(f\"  1000 vs 1000.1 (similar m/z):   {np.dot(encode_1000, encode_1000_1):.2f}\")\n",
    "print(f\"  1000 vs 126.127 (different m/z): {np.dot(encode_1000, encode_126):.2f}\")\n",
    "print(\"\\n→ Similar m/z values have higher dot products (more similar encodings)\")\n",
    "\n",
    "# Visualize the encodings\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "for ax, (enc, title) in zip(axes, [\n",
    "    (encode_1000, \"Positional Encoding — m/z = 1000\"),\n",
    "    (encode_1000_1, \"Positional Encoding — m/z = 1000.1 (very similar)\"),\n",
    "    (encode_126, \"Positional Encoding — m/z = 126.127 (very different)\")\n",
    "]):\n",
    "    ax.scatter(range(len(enc)), enc, s=5, alpha=0.7)\n",
    "    ax.axvline(x=256, color='red', linestyle='--', alpha=0.5, label='sine | cosine boundary')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Dimension (i)\")\n",
    "    ax.set_ylabel(\"Encoded Value\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdedac",
   "metadata": {},
   "source": [
    "### 3.10.1 Interpreting the Encoding Patterns\n",
    "\n",
    "Looking at the scatter plots above, we can see a clear pattern:\n",
    "\n",
    "| Dimension Range | Wavelength | Information Captured |\n",
    "|-----------------|------------|---------------------|\n",
    "| 0–50 (sine), 256–306 (cosine) | Short | High-resolution, changes rapidly with small m/z shifts |\n",
    "| 200–255 (sine), 456–511 (cosine) | Long | Low-resolution, stable across nearby m/z values |\n",
    "\n",
    "**Some Corollaries:**\n",
    "- The left side of each plot (fine wavelengths) looks like noise. Recall our red wave? It was oscillating so fast that small m/z changes cause large value changes. Same idea here. Regardless of how fast it is though, more dimensions gives the model a clearer picture of what's happening at each m/z value.\n",
    "- The right side (coarse wavelengths) shows smooth patterns - these change only for large m/z differences\n",
    "\n",
    "Let's zoom in to see this more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86abb3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compare LOW-resolution dimensions (coarse wavelengths: 200-255)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(range(200, 256), encode_1000[200:256], label='m/z = 1000', alpha=0.7)\n",
    "axes[0].scatter(range(200, 256), encode_1000_1[200:256], label='m/z = 1000.1', alpha=0.7)\n",
    "axes[0].set_title(\"COARSE Wavelengths (dims 200-255)\\nShould be similar for nearby m/z\")\n",
    "axes[0].set_xlabel(\"Dimension (i)\")\n",
    "axes[0].set_ylabel(\"Value\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(range(200, 256), encode_1000[200:256], label='m/z = 1000', alpha=0.7)\n",
    "axes[1].scatter(range(200, 256), encode_126[200:256], label='m/z = 126.127', alpha=0.7)\n",
    "axes[1].set_title(\"COARSE Wavelengths (dims 200-255)\\nShould be different for distant m/z\")\n",
    "axes[1].set_xlabel(\"Dimension (i)\")\n",
    "axes[1].set_ylabel(\"Value\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantify with cosine similarity\n",
    "sim_nearby = cosine_similarity([encode_1000[200:256]], [encode_1000_1[200:256]])[0][0]\n",
    "sim_distant = cosine_similarity([encode_1000[200:256]], [encode_126[200:256]])[0][0]\n",
    "print(f\"Cosine similarity in coarse dimensions:\")\n",
    "print(f\"  1000 vs 1000.1: {sim_nearby:.4f} (identical!)\")\n",
    "print(f\"  1000 vs 126.127: {sim_distant:.4f} (very different)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare HIGH-resolution dimensions (fine wavelengths: 0-56)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(range(0, 57), encode_1000[0:57], label='m/z = 1000', alpha=0.7)\n",
    "axes[0].scatter(range(0, 57), encode_1000_1[0:57], label='m/z = 1000.1', alpha=0.7)\n",
    "axes[0].set_title(\"Fine Wavelengths (dims 0-56)\\nCan distinguish SMALL m/z differences\")\n",
    "axes[0].set_xlabel(\"Dimension (i)\")\n",
    "axes[0].set_ylabel(\"Value\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(range(0, 57), encode_1000[0:57], label='m/z = 1000', alpha=0.7)\n",
    "axes[1].scatter(range(0, 57), encode_126[0:57], label='m/z = 126.127', alpha=0.7)\n",
    "axes[1].set_title(\"Fine Wavelengths (dims 0-56)\\nHighly variable for all m/z\")\n",
    "axes[1].set_xlabel(\"Dimension (i)\")\n",
    "axes[1].set_ylabel(\"Value\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantify with cosine similarity\n",
    "sim_nearby_fine = cosine_similarity([encode_1000[0:57]], [encode_1000_1[0:57]])[0][0]\n",
    "sim_distant_fine = cosine_similarity([encode_1000[0:57]], [encode_126[0:57]])[0][0]\n",
    "print(f\"Cosine similarity in FINE dimensions (0-56):\")\n",
    "print(f\"  1000 vs 1000.1: {sim_nearby_fine:.4f}\")\n",
    "print(f\"  1000 vs 126.127: {sim_distant_fine:.4f}\")\n",
    "print(f\"\\n→ Fine wavelengths detect the 0.1 Da difference that coarse wavelengths missed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902e253",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part III: From Encoding to Prediction\n",
    "\n",
    "## 3.11 Casanovo's Transformer Architecture\n",
    "\n",
    "Now that we understand how peaks are encoded, let's see how Casanovo uses these encodings to predict peptide sequences.\n",
    "\n",
    "<img src='Casanovo Transformer Architecture.webp' width=500 height=500>\n",
    "\n",
    "The model has three inputs:\n",
    "1. **Encoded spectrum** — our 512-dimensional vectors for each peak\n",
    "2. **Encoded precursor** — mass and charge of the intact peptide (sets the mass budget)\n",
    "3. **Previously predicted amino acids** — enables autoregressive generation\n",
    "\n",
    "---\n",
    "\n",
    "## 3.12 The Translation Analogy\n",
    "\n",
    "Casanovo works like a language translator, but for molecules:\n",
    "\n",
    "### 3.12.1 Transformer Encoder-Decoder\n",
    "\n",
    "| English → Spanish | Spectrum → Peptide |\n",
    "|------------------|-------------------|\n",
    "| Read English sentence | Read all peak encodings |\n",
    "| Build encoded representation of meaning | Build representation of peak relationships |\n",
    "| Decoder predicts first Spanish word | Decoder predicts first amino acid |\n",
    "| Continue, conditioning on previous words | Continue, conditioning on previous amino acids |\n",
    "\n",
    "**The first prediction is special:** Without previous amino acids, Casanovo uses the precursor mass/charge to make its first guess. Then each subsequent amino acid is predicted based on:\n",
    "- All peak encodings (via attention)\n",
    "- The sequence generated so far\n",
    "\n",
    "---\n",
    "\n",
    "### 3.12.2 Linear + Softmax Output\n",
    "\n",
    "After the decoder produces a representation for the next position:\n",
    "\n",
    "1. **Linear layer:** Computes a score for each of the ~27 possible outputs (20 amino acids + modifications + stop token)\n",
    "2. **Softmax:** Converts scores to probabilities\n",
    "\n",
    "Example output: `[Gly: 0.02, Ala: 0.05, ..., Lys: 0.71, ...]` → Predict **Lysine (K)**\n",
    "\n",
    "---\n",
    "\n",
    "### 3.12.3 Beam Search Decoding\n",
    "\n",
    "> Beam Search is just a simple backtracking with a fancy name. \n",
    "\n",
    "Instead of greedily picking the most likely amino acid at each step, beam search keeps the **top-k** candidates:\n",
    "\n",
    "<img src='Beam Search Ex.webp' alt=\"Beam search visualization\" width=600 height=300>\n",
    "\n",
    "**How it works:**\n",
    "1. Start with k=5 partial sequences\n",
    "2. Extend each by all possible amino acids\n",
    "3. Keep only the top-k most probable extensions\n",
    "4. Prune sequences that exceed precursor mass\n",
    "5. When all sequences terminate, return the highest-probability one\n",
    "\n",
    "**Example calculation:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(ABC) &= P(A) \\times P(B|A) \\times P(C|AB) \\\\\n",
    "       &= 0.5 \\times 0.4 \\times 0.8 = 0.16 \\\\\n",
    "P(AED) &= P(A) \\times P(E|A) \\times P(D|AE)  \\\\\n",
    "       &= 0.5 \\times 0.5 \\times 0.8 = 0.20\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "→ \"AED\" wins over \"ABC\" even though individual probabilities varied!\n",
    "\n",
    "> Beam search finds the **globally best sequence**, not just locally optimal choices (greedy). This is crucial because amino acid probabilities depend heavily on context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8d0ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.13 Summary: Why This Works\n",
    "\n",
    "Let's connect everything back to the original problem:\n",
    "\n",
    "### The Problem\n",
    "We need to predict a peptide sequence from MS/MS peaks without a reference database.\n",
    "\n",
    "### The Solution\n",
    "1. **Positional Encoding** transforms raw m/z values into rich 512-dimensional vectors that:\n",
    "   - Capture both fine-scale (isotopes) and coarse-scale (peptide regions) information\n",
    "   - Enable linear transformations for learning relative positions\n",
    "   - Allow the model to understand \"this peak is 128 Da away from that one\"\n",
    "\n",
    "2. **Transformer Architecture** processes these encodings through:\n",
    "   - Attention mechanisms that learn peak-to-peak relationships\n",
    "   - Autoregressive decoding that builds sequences one amino acid at a time\n",
    "\n",
    "3. **Beam Search** finds the globally optimal sequence by exploring multiple hypotheses simultaneously.\n",
    "\n",
    "### The Lego Analogy (Complete)\n",
    "\n",
    "| Step | Lego | Casanovo |\n",
    "|------|------|----------|\n",
    "| Representation | Measure each piece precisely | Encode each peak with multi-scale wavelengths |\n",
    "| Pattern Recognition | \"2x4 bricks connect to plates here\" | \"b-ions follow this pattern\" |\n",
    "| Sequential Assembly | Build one piece at a time | Predict one amino acid at a time |\n",
    "| Quality Control | Check if pieces fit | Verify mass within tolerance |\n",
    "| Backtracking | Try different assembly orders | Beam search explores alternatives |\n",
    "\n",
    "---\n",
    "\n",
    "## 3.14 Further Reading\n",
    "\n",
    "- **Casanovo Paper:** [Sequence-to-sequence translation from mass spectra to peptides with a transformer model](https://www.nature.com/articles/s41467-024-49731-x)\n",
    "- **Positional Encoding Deep Dive:** [Linear Relationships in Transformer Positional Encoding](https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/)\n",
    "- **Original Transformer Architecture:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
